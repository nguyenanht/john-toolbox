{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "690be1c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler, normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f451fbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/90971606.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  requests_train = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/requests_train.csv',\n",
      "/tmp/ipykernel_18/90971606.py:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  requests_test = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/requests_test.csv',\n",
      "/tmp/ipykernel_18/90971606.py:12: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  individuals_train = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/individuals_train.csv',\n",
      "/tmp/ipykernel_18/90971606.py:16: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  individuals_test = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/individuals_test.csv',\n"
     ]
    }
   ],
   "source": [
    "# Get datasets\n",
    "\n",
    "requests_train = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/requests_train.csv',\n",
    "                             sep=',',\n",
    "                             low_memory=False,\n",
    "                             error_bad_lines=False)\n",
    "requests_test = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/requests_test.csv',\n",
    "                            sep=',',\n",
    "                            low_memory=False,\n",
    "                            error_bad_lines=False)\n",
    "\n",
    "individuals_train = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/individuals_train.csv',\n",
    "                                sep=',',\n",
    "                                low_memory=False,\n",
    "                                error_bad_lines=False)\n",
    "individuals_test = pd.read_csv(filepath_or_buffer='/work/data/housing_assistance/individuals_test.csv',\n",
    "                               sep=',',\n",
    "                               low_memory=False,\n",
    "                               error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fbe76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95b6c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "654da10d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cyclically encodes given data using sin and cos\n",
    "\n",
    "def cyclical_encoding(data, col, col_name, max_val):\n",
    "    data[col_name + '_sin'] = np.sin(2 * np.pi * col/max_val)\n",
    "    data[col_name + '_cos'] = np.cos(2 * np.pi * col/max_val)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7c150f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Encodes timestamp by seperating year, month, day and cyclically encoding month and day\n",
    "\n",
    "def date_encoding(data, var):\n",
    "    data[f'{var}_creation_date'] = pd.to_datetime(data[f'{var}_creation_date'], yearfirst=True)\n",
    "    data[f'{var}_creation_year'] = data[f'{var}_creation_date'].dt.year\n",
    "    data = cyclical_encoding(data, data[f'{var}_creation_date'].dt.month, f'{var}_creation_month', 12.0) #to do: check\n",
    "    data = cyclical_encoding(data, data[f'{var}_creation_date'].dt.day, f'{var}_creation_day', 365.0)\n",
    "    data.drop(columns=[f'{var}_creation_date'], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd4c5b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_initial_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ad3b4fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_initial_dataset(request_df, indiv_df, request_drops, indiv_drops):\n",
    "    # Encode timestamps\n",
    "    request_df = date_encoding(request_df, \"answer\")\n",
    "    request_df = date_encoding(request_df, \"group\")\n",
    "    request_df = date_encoding(request_df, \"request\")\n",
    "\n",
    "    indiv_df = date_encoding(indiv_df, \"individual\")\n",
    "\n",
    "    # Drop unneeded columns- if too many modalities or new data present in test set and not training set\n",
    "    request_df.drop(request_drops, axis=1, inplace=True)\n",
    "    indiv_df.drop(indiv_drops, axis=1, inplace=True)\n",
    "\n",
    "    indiv_df[\"childcare_center_supervision\"] = indiv_df[\n",
    "        \"childcare_center_supervision\"\n",
    "    ].fillna(\"x\")\n",
    "    indiv_df[\"disabled_worker_certification\"] = indiv_df[\n",
    "        \"disabled_worker_certification\"\n",
    "    ].fillna(\"x\")\n",
    "\n",
    "    # Group individuals\n",
    "    indiv_df = indiv_df.groupby([\"request_id\"]).agg([\"mean\", \"median\"])\n",
    "\n",
    "    # Flatten individuals data\n",
    "    indiv_df.columns = [\" \".join(col).strip() for col in indiv_df.columns.values]\n",
    "\n",
    "    # Merge request and individual data\n",
    "    X_train = pd.merge(request_df, indiv_df, how=\"left\", on=\"request_id\")\n",
    "    return X_train, indiv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282967c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63292476",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_train(X, numeric_features, categorical_features):\n",
    "    cols = numeric_features + categorical_features\n",
    "    X = X[cols].copy()\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\n",
    "                \"ohe\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\", max_categories=30, sparse=False),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # to do: check if need mode or \"unknown\"\n",
    "\n",
    "    X_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    X_transformed = X_preprocessor.fit_transform(X)\n",
    "\n",
    "    return X_transformed, X_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b0120",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02f55f5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_prediction(X, pipeline):\n",
    "    return pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4848f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CONSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4450a7fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "request_drops = [\n",
    "    \"granted_number_of_nights\",\n",
    "    \"group_id\",\n",
    "    \"group_main_requester_id\",\n",
    "    \"housing_situation_label\",\n",
    "    \"request_backoffice_creator_id\",\n",
    "    \"social_situation_id\",\n",
    "    \"victim_of_violence_type\",\n",
    "]\n",
    "\n",
    "indiv_drops = [\n",
    "    \"individual_id\",\n",
    "    \"housing_situation_2_label\",\n",
    "    \"individual_role\",\n",
    "    \"individual_role_2_label\",\n",
    "    \"marital_status_label\",\n",
    "]\n",
    "\n",
    "TARGET = \"granted_number_of_nights\"\n",
    "\n",
    "req_numeric_features = [\n",
    "    \"answer_creation_year\",\n",
    "    \"answer_creation_month_sin\",\n",
    "    \"answer_creation_month_cos\",\n",
    "    \"answer_creation_day_sin\",\n",
    "    \"answer_creation_day_cos\",\n",
    "    \"group_composition_id\",\n",
    "    \"group_creation_year\",\n",
    "    \"group_creation_month_sin\",\n",
    "    \"group_creation_month_cos\",\n",
    "    \"group_creation_day_sin\",\n",
    "    \"group_creation_day_cos\",\n",
    "    \"housing_situation_id\",\n",
    "    \"number_of_underage\",\n",
    "    \"request_creation_year\",\n",
    "    \"request_creation_month_sin\",\n",
    "    \"request_creation_month_cos\",\n",
    "    \"request_creation_day_sin\",\n",
    "    \"request_creation_day_cos\",\n",
    "]\n",
    "\n",
    "\n",
    "req_categorical_features = [\n",
    "    \"animal_presence\",\n",
    "    \"child_to_come\",\n",
    "    \"group_type\",\n",
    "    \"long_term_housing_request\",\n",
    "    \"requester_type\",\n",
    "    \"town\",\n",
    "    \"victim_of_violence\",\n",
    "    \"district\",\n",
    "    \"child_situation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee16b1d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f64166",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### copy df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "687bea23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# request\n",
    "req_train = requests_train.copy()\n",
    "req_train_outputs = requests_train[TARGET].copy().values\n",
    "\n",
    "req_test = requests_test.copy()\n",
    "\n",
    "# indiv\n",
    "indiv_train = individuals_train.copy()\n",
    "indiv_test = individuals_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8ddbbd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238191, 24)\n",
      "(59548, 24)\n"
     ]
    }
   ],
   "source": [
    "print(req_train.shape)\n",
    "print(req_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d9cef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### preprocess_initial_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8adfd5e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/3360056888.py:21: FutureWarning: ['childcare_center_supervision', 'disabled_worker_certification', 'gender', 'pregnancy'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  indiv_df = indiv_df.groupby([\"request_id\"]).agg([\"mean\", \"median\"])\n",
      "/tmp/ipykernel_18/3360056888.py:21: FutureWarning: ['childcare_center_supervision', 'disabled_worker_certification', 'gender', 'pregnancy'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  indiv_df = indiv_df.groupby([\"request_id\"]).agg([\"mean\", \"median\"])\n"
     ]
    }
   ],
   "source": [
    "X_train_validate, indiv_train = preprocess_initial_dataset(\n",
    "    req_train,\n",
    "    indiv_train,\n",
    "    request_drops,\n",
    "    indiv_drops,\n",
    ")\n",
    "\n",
    "X_test, indiv_test = preprocess_initial_dataset(\n",
    "    req_test,\n",
    "    indiv_test,\n",
    "    request_drops,\n",
    "    indiv_drops,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c84fa457",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238191, 49)\n",
      "(59548, 49)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_validate.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123e154",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42965879",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split training dataset\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train_validate, req_train_outputs, test_size=0.20, \n",
    "                                                            stratify=req_train_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fe3c2b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190552, 49)\n",
      "(47639, 49)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed821d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52d26725",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = req_numeric_features + indiv_train.columns.values.tolist()\n",
    "categorical_features = req_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46ffaf5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_transformed, pipeline = preprocess_train(\n",
    "    X_train,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30c39c71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_validate_transformed = preprocess_prediction(X_validate, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce726095",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test_transformed = preprocess_prediction(X_test, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed0489a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190552, 119)\n",
      "(47639, 119)\n",
      "(59548, 119)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_transformed.shape)\n",
    "print(X_validate_transformed.shape)\n",
    "print(X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbee4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45207e2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prep data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82fb5c12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db6aee3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# transform to torch tensors\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train_transformed)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "y_train_tensor = y_train_tensor.type(torch.LongTensor)\n",
    "# y_train_tensor = .reshape(-1, 1)\n",
    "\n",
    "X_validate_tensor = torch.Tensor(X_validate_transformed)\n",
    "y_validate_tensor = torch.Tensor(y_validate)\n",
    "y_validate_tensor = y_validate_tensor.type(torch.LongTensor)\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d46fefc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "validate_dataset = TensorDataset(X_validate_tensor, y_validate_tensor)\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    X_test_tensor, torch.Tensor(np.random.randint(4, size=X_test_tensor.shape[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab1cd23f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 100\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da533a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a41c47b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Create model\n",
    "\n",
    "# input_size = X_train.shape[1] # + 1 for bias\n",
    "# hidden_sizes = [100, 50]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "#     nn.LogSoftmax(dim=1), # activation function for classification, normalizes values along axis 1\n",
    "# )\n",
    "# # drop out layer\n",
    "# # activation func, find diff funcs that work\n",
    "# # hidden layer- size, amounr, etc\n",
    "# # loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b14ec57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Model 2- hidden sizes\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_sizes = [45, 10]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "#     nn.LogSoftmax(dim=1), \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48aa60e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Model 3- dropout layers\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_sizes = [45, 10]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.Dropout(),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.Dropout(),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "#     nn.LogSoftmax(dim=1), \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fd2ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c158df36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model 4- Prevent dying neurons\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_sizes = [512, 128]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.LeakyReLU(),\n",
    "#     torch.nn.Dropout(0.1),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.LeakyReLU(),\n",
    "#     torch.nn.Dropout(0.4),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6b13e4d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Model 5- different final activation func\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_sizes = [45, 10]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "#     nn.Softmax(dim=1), \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "347cff2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Model 5- different final activation func\n",
    "\n",
    "# input_size = X_train.shape[1]\n",
    "# hidden_sizes = [45, 10]\n",
    "# output_size = 4\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "#     nn.Softmax(dim=1), \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cdf4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "03dd944b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, h0, h1, h2, output_size):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, h0)\n",
    "        self.dp1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(h0, h1)\n",
    "        self.dp2 = torch.nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(h1, h2)\n",
    "        self.dp3 = torch.nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(h2, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # layer1\n",
    "        out = F.leaky_relu(self.fc1(X))\n",
    "        out = self.dp1(out)\n",
    "        # layer2\n",
    "        out = F.leaky_relu(self.fc2(out))\n",
    "        out = self.dp2(out)\n",
    "        # layer3\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.dp3(out)\n",
    "        # layer3\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b09eaabc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, h0, h1, h2, output_size):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, h0)\n",
    "        self.dp1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(h0, h1)\n",
    "        self.dp2 = torch.nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(h1, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # layer1\n",
    "        out = F.leaky_relu(self.fc1(X))\n",
    "        out = self.dp1(out)\n",
    "        # layer2\n",
    "        out = F.leaky_relu(self.fc2(out))\n",
    "        out = self.dp2(out)\n",
    "        # layer3\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e905858",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: tensor([   1.,   10.,  100., 1000.])\n"
     ]
    }
   ],
   "source": [
    "#criterion = nn.NLLLoss() # to do: account for unbalanced\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "\n",
    "weight = torch.Tensor([10**el for el in range(4)])\n",
    "print(f\"weight: {weight}\")\n",
    "\n",
    "\n",
    "# Define model\n",
    "input_size = X_train_transformed.shape[1]\n",
    "hidden_sizes = [150, 60, 30]\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a489bce9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(input_size, hidden_sizes[0], hidden_sizes[1], hidden_sizes[2], output_size)\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "288d54f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = torch.nn.Sequential(\n",
    "#     nn.Linear(input_size, hidden_sizes[0]),\n",
    "#     nn.LeakyReLU(),\n",
    "# #     torch.nn.Dropout(0.2),\n",
    "#     nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "#     nn.LeakyReLU(),\n",
    "# #     torch.nn.Dropout(0.2),\n",
    "#     nn.Linear(hidden_sizes[1], output_size),\n",
    "# )\n",
    "# model = model.cuda()\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdcba6c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=119, out_features=150, bias=True)\n",
      "  (dp1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=150, out_features=60, bias=True)\n",
      "  (dp2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=60, out_features=4, bias=True)\n",
      ")\n",
      "0.9707436874305589\n",
      "0.8134481151669373\n",
      "0.7645778346255594\n",
      "0.7405097114952264\n",
      "0.7282349711356358\n",
      "0.7188937951109967\n",
      "0.7129046597986132\n",
      "0.7070022132032691\n",
      "0.7004978077140209\n",
      "0.6945794878482568\n",
      "0.6890070446359399\n",
      "0.6850863202530715\n",
      "0.679951983352523\n",
      "0.6760937691766843\n",
      "0.6727434562798688\n",
      "0.6685160892113559\n",
      "0.6683365847888048\n",
      "0.6636286321304278\n",
      "0.6602959308269016\n",
      "0.6562066148235565\n",
      "0.6524591565538679\n",
      "0.651687609045478\n",
      "0.6465869450713004\n",
      "0.6455509856046184\n",
      "0.6418374467011764\n",
      "0.638337201322551\n",
      "0.6396013053773207\n",
      "0.6368685240885643\n",
      "0.6351564720992277\n",
      "0.6296330052667375\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 30\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weight.cuda())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) #lr=0.001\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader):     \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Perform forward pass, backpropogation, and optimizer step\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # model learning\n",
    "        \n",
    "        optimizer.step() # optimizing weights\n",
    "        train_loss += loss.item()\n",
    "    print(train_loss / len(train_dataloader))\n",
    "\n",
    "    # to do: validate later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "attended-middle",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=119, out_features=150, bias=True)\n",
      "  (dp1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=150, out_features=60, bias=True)\n",
      "  (dp2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=60, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "preceding-withdrawal",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# valid_loss = 0.0\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i, data in enumerate(validate_dataloader):\n",
    "#         inputs, labels = data\n",
    "#         outputs = model(inputs)\n",
    "#         labels = labels.squeeze_()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         valid_loss = loss.item()\n",
    "        \n",
    "# train_loss /= len(X_train)\n",
    "# valid_loss /= len(X_validate)\n",
    "# print(f'Epoch: {epoch+1}/{epoch_range} \\nTraining loss: {train_loss} \\nValidation Loss: {valid_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-convergence",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "suburban-belief",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, inputs in enumerate(test_dataloader):\n",
    "        # print(inputs)\n",
    "        model.eval()\n",
    "        \n",
    "        X = inputs[0].to(device)  \n",
    "        \n",
    "        outputs = model.forward(X)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        # print(outputs)\n",
    "#         pred = outputs.argmax(1)\n",
    "        pred = outputs.cpu().data.numpy()\n",
    "        #print(pred)\n",
    "        preds.append(pred)\n",
    "    \n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aea1daec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_flatten = np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0787fbe3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48310304, 0.07251088, 0.36292577, 0.0814603 ],\n",
       "       [0.03820433, 0.09640943, 0.45239067, 0.4129955 ],\n",
       "       [0.00680584, 0.04137591, 0.42570946, 0.52610874],\n",
       "       ...,\n",
       "       [0.00501883, 0.04434477, 0.9468825 , 0.00375384],\n",
       "       [0.01922334, 0.07781976, 0.6810838 , 0.22187316],\n",
       "       [0.90508896, 0.03084334, 0.05370409, 0.01036356]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c944f9da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48310304, 0.07251088, 0.36292577, 0.0814603 ],\n",
       "       [0.03820433, 0.09640943, 0.45239067, 0.4129955 ],\n",
       "       [0.00680584, 0.04137591, 0.42570946, 0.52610874],\n",
       "       ...,\n",
       "       [0.00501883, 0.04434477, 0.9468825 , 0.00375384],\n",
       "       [0.01922334, 0.07781976, 0.6810838 , 0.22187316],\n",
       "       [0.90508896, 0.03084334, 0.05370409, 0.01036356]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a63a9d12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert len(X_test) == len(p_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "third-swiss",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define test scorer\n",
    "\n",
    "def competition_scorer(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred, sample_weight=10**y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "black-driving",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 1 0]\n",
      "Random test score: 1.6673729796281185\n"
     ]
    }
   ],
   "source": [
    "random_preds = np.random.uniform(size=(requests_test.shape[0], 4))\n",
    "y_true = requests_test.granted_number_of_nights.values\n",
    "print(y_true)\n",
    "\n",
    "random_score = competition_scorer(y_true, random_preds)\n",
    "print(f'Random test score: {random_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ab6844f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 1, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f98eea7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/2784755074.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  preds = np.array(preds)\n"
     ]
    }
   ],
   "source": [
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "physical-korean",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test score: 0.6586562687987075\n"
     ]
    }
   ],
   "source": [
    "model_score = competition_scorer(y_true, p_flatten)\n",
    "print(f'Model test score: {model_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ancient-writer",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.3862943649291992"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
